

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>spotlight.layers &mdash; Spotlight v0.1.2 documentation</title>
  

  
  
  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  

  
    <link rel="stylesheet" href="../../_static/css/spotlight_theme.css" type="text/css" />
  

  
        <link rel="index" title="Index"
              href="../../genindex.html"/>
        <link rel="search" title="Search" href="../../search.html"/>
    <link rel="top" title="Spotlight v0.1.2 documentation" href="../../index.html"/>
        <link rel="up" title="Module code" href="../index.html"/> 

  
  <script src="../../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../../index.html">
          

          
            
            <img src="../../_static/spotlight.svg" class="logo" />
          
          </a>

          
            
            
              <div class="version">
                v0.1.2
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../interactions.html">Interactions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../datasets/datasets.html">Datasets</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../datasets/synthetic.html">Synthetic</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../datasets/movielens.html">Movielens</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../datasets/goodbooks.html">Goodbooks</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../cross_validation.html">Cross validation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../sequence/sequence.html">Sequence models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../sequence/implicit.html">Implicit feedback models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../sequence/representations.html">Sequence representations</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../factorization/factorization.html">Factorization models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../factorization/implicit.html">Implicit feedback models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../factorization/explicit.html">Explicit feedback models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../factorization/representations.html">Latent representations</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../layers.html">Layers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../losses.html">Loss functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../evaluation.html">Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../sampling.html">Sampling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../serialization.html">Model Serialization</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../serialization.html#saving-and-loading-the-model">Saving and loading the model</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../changelog.html">Changelog</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../changelog.html#unreleased-unreleased">unreleased (unreleased)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../changelog.html#added">Added</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../changelog.html#changed">Changed</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../changelog.html#v0-1-2-2017-09-10">v0.1.2 (2017-09-10)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../changelog.html#id1">Added</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../changelog.html#id2">Changed</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../changelog.html#fixed">Fixed</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Spotlight</a>
        
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../index.html">Module code</a> &raquo;</li>
        
      <li>spotlight.layers</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for spotlight.layers</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Embedding layers useful for recommender models.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">from</span> <span class="nn">sklearn.utils</span> <span class="k">import</span> <span class="n">murmurhash3_32</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="kn">from</span> <span class="nn">torch.autograd</span> <span class="k">import</span> <span class="n">Variable</span>


<span class="n">SEEDS</span> <span class="o">=</span> <span class="p">[</span>
    <span class="mi">179424941</span><span class="p">,</span> <span class="mi">179425457</span><span class="p">,</span> <span class="mi">179425907</span><span class="p">,</span> <span class="mi">179426369</span><span class="p">,</span>
    <span class="mi">179424977</span><span class="p">,</span> <span class="mi">179425517</span><span class="p">,</span> <span class="mi">179425943</span><span class="p">,</span> <span class="mi">179426407</span><span class="p">,</span>
    <span class="mi">179424989</span><span class="p">,</span> <span class="mi">179425529</span><span class="p">,</span> <span class="mi">179425993</span><span class="p">,</span> <span class="mi">179426447</span><span class="p">,</span>
    <span class="mi">179425003</span><span class="p">,</span> <span class="mi">179425537</span><span class="p">,</span> <span class="mi">179426003</span><span class="p">,</span> <span class="mi">179426453</span><span class="p">,</span>
    <span class="mi">179425019</span><span class="p">,</span> <span class="mi">179425559</span><span class="p">,</span> <span class="mi">179426029</span><span class="p">,</span> <span class="mi">179426491</span><span class="p">,</span>
    <span class="mi">179425027</span><span class="p">,</span> <span class="mi">179425579</span><span class="p">,</span> <span class="mi">179426081</span><span class="p">,</span> <span class="mi">179426549</span>
<span class="p">]</span>


<div class="viewcode-block" id="ScaledEmbedding"><a class="viewcode-back" href="../../layers.html#spotlight.layers.ScaledEmbedding">[docs]</a><span class="k">class</span> <span class="nc">ScaledEmbedding</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Embedding layer that initialises its values</span>
<span class="sd">    to using a normal variable scaled by the inverse</span>
<span class="sd">    of the emedding dimension.</span>
<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="ScaledEmbedding.reset_parameters"><a class="viewcode-back" href="../../layers.html#spotlight.layers.ScaledEmbedding.reset_parameters">[docs]</a>    <span class="k">def</span> <span class="nf">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialize parameters.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding_dim</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span><span class="p">]</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="ZeroEmbedding"><a class="viewcode-back" href="../../layers.html#spotlight.layers.ZeroEmbedding">[docs]</a><span class="k">class</span> <span class="nc">ZeroEmbedding</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Embedding layer that initialises its values</span>
<span class="sd">    to using a normal variable scaled by the inverse</span>
<span class="sd">    of the emedding dimension.</span>

<span class="sd">    Used for biases.</span>
<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="ZeroEmbedding.reset_parameters"><a class="viewcode-back" href="../../layers.html#spotlight.layers.ZeroEmbedding.reset_parameters">[docs]</a>    <span class="k">def</span> <span class="nf">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialize parameters.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span><span class="p">]</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="ScaledEmbeddingBag"><a class="viewcode-back" href="../../layers.html#spotlight.layers.ScaledEmbeddingBag">[docs]</a><span class="k">class</span> <span class="nc">ScaledEmbeddingBag</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">EmbeddingBag</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    EmbeddingBag layer that initialises its values</span>
<span class="sd">    to using a normal variable scaled by the inverse</span>
<span class="sd">    of the emedding dimension.</span>
<span class="sd">    &quot;&quot;&quot;</span>

<div class="viewcode-block" id="ScaledEmbeddingBag.reset_parameters"><a class="viewcode-back" href="../../layers.html#spotlight.layers.ScaledEmbeddingBag.reset_parameters">[docs]</a>    <span class="k">def</span> <span class="nf">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialize parameters.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding_dim</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="BloomEmbedding"><a class="viewcode-back" href="../../layers.html#spotlight.layers.BloomEmbedding">[docs]</a><span class="k">class</span> <span class="nc">BloomEmbedding</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    An embedding layer that compresses the number of embedding</span>
<span class="sd">    parameters required by using bloom filter-like hashing.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>

<span class="sd">    num_embeddings: int</span>
<span class="sd">        Number of entities to be represented.</span>
<span class="sd">    embedding_dim: int</span>
<span class="sd">        Latent dimension of the embedding.</span>
<span class="sd">    compression_ratio: float, optional</span>
<span class="sd">        The underlying number of rows in the embedding layer</span>
<span class="sd">        after compression. Numbers below 1.0 will use more</span>
<span class="sd">        and more compression, reducing the number of parameters</span>
<span class="sd">        in the layer.</span>
<span class="sd">    num_hash_functions: int, optional</span>
<span class="sd">        Number of hash functions used to compute the bloom filter indices.</span>
<span class="sd">    bag: bool, optional</span>
<span class="sd">        Whether to use the ``EmbeddingBag`` layer for the underlying embedding.</span>
<span class="sd">        This should be faster in principle, but currently seems to perform</span>
<span class="sd">        very poorly.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>

<span class="sd">    Large embedding layers are a performance problem for fitting models:</span>
<span class="sd">    even though the gradients are sparse (only a handful of user and item</span>
<span class="sd">    vectors need parameter updates in every minibatch), PyTorch updates</span>
<span class="sd">    the entire embedding layer at every backward pass. Computation time</span>
<span class="sd">    is then wasted on applying zero gradient steps to whole embedding matrix.</span>

<span class="sd">    To alleviate this problem, we can use a smaller underlying embedding layer,</span>
<span class="sd">    and probabilistically hash users and items into that smaller space. With</span>
<span class="sd">    good hash functions, collisions should be rare, and we should observe</span>
<span class="sd">    fitting speedups without a decrease in accuracy.</span>

<span class="sd">    The idea follows the RecSys 2017 &quot;Getting recommenders fit&quot;[1]_</span>
<span class="sd">    paper. The authors use a bloom-filter-like approach to hashing. Their approach</span>
<span class="sd">    uses one-hot encoded inputs followed by fully connected layers as</span>
<span class="sd">    well as softmax layers for the output, and their hashing reduces the</span>
<span class="sd">    size of the fully connected layers rather than embedding layers as</span>
<span class="sd">    implemented here; mathematically, however, the two formulations are</span>
<span class="sd">    identical.</span>

<span class="sd">    The hash function used is murmurhash3, hashing the indices with a different</span>
<span class="sd">    seed for every hash function, modulo the size of the compressed embedding layer.</span>
<span class="sd">    The hash mapping is computed once at the start of training, and indexed</span>
<span class="sd">    into for every minibatch.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>

<span class="sd">    .. [1] Serra, Joan, and Alexandros Karatzoglou.</span>
<span class="sd">       &quot;Getting deep recommenders fit: Bloom embeddings</span>
<span class="sd">       for sparse binary input/output networks.&quot;</span>
<span class="sd">       arXiv preprint arXiv:1706.03993 (2017).</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_embeddings</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span>
                 <span class="n">compression_ratio</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
                 <span class="n">num_hash_functions</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                 <span class="n">bag</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">padding_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">BloomEmbedding</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">num_embeddings</span> <span class="o">=</span> <span class="n">num_embeddings</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding_dim</span> <span class="o">=</span> <span class="n">embedding_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">compression_ratio</span> <span class="o">=</span> <span class="n">compression_ratio</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">compressed_num_embeddings</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">compression_ratio</span> <span class="o">*</span>
                                             <span class="n">num_embeddings</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_hash_functions</span> <span class="o">=</span> <span class="n">num_hash_functions</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span> <span class="o">=</span> <span class="n">padding_idx</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_bag</span> <span class="o">=</span> <span class="n">bag</span>

        <span class="k">if</span> <span class="n">num_hash_functions</span> <span class="o">&gt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">SEEDS</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Can use at most </span><span class="si">{}</span><span class="s1"> hash functions (</span><span class="si">{}</span><span class="s1"> requested)&#39;</span>
                             <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">SEEDS</span><span class="p">),</span> <span class="n">num_hash_functions</span><span class="p">))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_masks</span> <span class="o">=</span> <span class="n">SEEDS</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">num_hash_functions</span><span class="p">]</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_bag</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">ScaledEmbeddingBag</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">compressed_num_embeddings</span><span class="p">,</span>
                                                 <span class="bp">self</span><span class="o">.</span><span class="n">embedding_dim</span><span class="p">,</span>
                                                 <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;sum&#39;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">ScaledEmbedding</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">compressed_num_embeddings</span><span class="p">,</span>
                                              <span class="bp">self</span><span class="o">.</span><span class="n">embedding_dim</span><span class="p">,</span>
                                              <span class="n">padding_idx</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span><span class="p">)</span>

        <span class="c1"># Hash cache. We pre-hash all the indices, and then just</span>
        <span class="c1"># map the indices to their pre-hashed values as we go</span>
        <span class="c1"># through the minibatches.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_hashes</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_offsets</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>

        <span class="k">return</span> <span class="p">(</span><span class="s1">&#39;&lt;BloomEmbedding (compression_ratio: </span><span class="si">{}</span><span class="s1">): </span><span class="si">{}</span><span class="s1">&gt;&#39;</span>
                <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">compression_ratio</span><span class="p">,</span>
                        <span class="nb">repr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span><span class="p">)))</span>

    <span class="k">def</span> <span class="nf">_get_hashed_indices</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">original_indices</span><span class="p">):</span>

        <span class="k">def</span> <span class="nf">_hash</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">seed</span><span class="p">):</span>

            <span class="c1"># TODO: integrate with padding index</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">murmurhash3_32</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
            <span class="n">result</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

            <span class="k">return</span> <span class="n">result</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">compressed_num_embeddings</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hashes</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_embeddings</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
            <span class="n">hashes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">_hash</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span>
                               <span class="k">for</span> <span class="n">seed</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_masks</span><span class="p">],</span>
                              <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span>
            <span class="k">assert</span> <span class="n">hashes</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">padding_idx</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">_hashes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">hashes</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">original_indices</span><span class="o">.</span><span class="n">is_cuda</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_hashes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_hashes</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

        <span class="n">hashed_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">index_select</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_hashes</span><span class="p">,</span>
                                            <span class="mi">0</span><span class="p">,</span>
                                            <span class="n">original_indices</span><span class="o">.</span><span class="n">squeeze</span><span class="p">())</span>

        <span class="k">return</span> <span class="n">hashed_indices</span>

<div class="viewcode-block" id="BloomEmbedding.forward"><a class="viewcode-back" href="../../layers.html#spotlight.layers.BloomEmbedding.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">indices</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Retrieve embeddings corresponding to indices.</span>

<span class="sd">        See documentation on PyTorch ``nn.Embedding`` for details.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="n">indices</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_size</span> <span class="o">=</span> <span class="n">indices</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_size</span> <span class="o">=</span> <span class="n">indices</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="mi">1</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">indices</span><span class="o">.</span><span class="n">is_contiguous</span><span class="p">():</span>
            <span class="n">indices</span> <span class="o">=</span> <span class="n">indices</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>

        <span class="n">indices</span> <span class="o">=</span> <span class="n">indices</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">seq_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_bag</span><span class="p">:</span>
            <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_offsets</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_offsets</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">!=</span> <span class="p">(</span><span class="n">batch_size</span> <span class="o">*</span> <span class="n">seq_size</span><span class="p">)):</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">_offsets</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span>
                                                      <span class="n">indices</span><span class="o">.</span><span class="n">numel</span><span class="p">(),</span>
                                                      <span class="n">indices</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">long</span><span class="p">())</span>

                <span class="k">if</span> <span class="n">indices</span><span class="o">.</span><span class="n">is_cuda</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_offsets</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_offsets</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

            <span class="n">hashed_indices</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_get_hashed_indices</span><span class="p">(</span><span class="n">indices</span><span class="p">))</span>
            <span class="n">embedding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span><span class="p">(</span><span class="n">hashed_indices</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">_offsets</span><span class="p">)</span>
            <span class="n">embedding</span> <span class="o">=</span> <span class="n">embedding</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">hashed_indices</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_get_hashed_indices</span><span class="p">(</span><span class="n">indices</span><span class="p">))</span>

            <span class="n">embedding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span><span class="p">(</span><span class="n">hashed_indices</span><span class="p">)</span>
            <span class="n">embedding</span> <span class="o">=</span> <span class="n">embedding</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">embedding</span> <span class="o">=</span> <span class="n">embedding</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">embedding</span></div></div>
</pre></div>

           </div>
           <div class="articleComments">
            
           </div>
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, Maciej Kula.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../../',
            VERSION:'v0.1.2',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../../_static/jquery.js"></script>
      <script type="text/javascript" src="../../_static/underscore.js"></script>
      <script type="text/javascript" src="../../_static/doctools.js"></script>

  

  
  
    <script type="text/javascript" src="../../_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>